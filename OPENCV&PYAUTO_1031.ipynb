{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 얼굴인식(normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n",
      "dd\n"
     ]
    }
   ],
   "source": [
    "import cv2, dlib, sys\n",
    "\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat') # 사람얼굴인식 최신모델 \n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)  # 화면 사이즈 조절 -> Droidcam is limited to 480p\n",
    "capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "\n",
    "while True:\n",
    "    ret, img = capture.read()\n",
    "    if not ret:\n",
    "        break;\n",
    "\n",
    "    try : # 얼굴인식 되면 = 사람이 있으면\n",
    "        faces = detector(img)\n",
    "        face = faces[0]\n",
    "        img = cv2.rectangle(img, pt1=(face.left(), face.top()), pt2=(face.right(), face.bottom()), color=(255,255,255),\n",
    "                   thickness=2, lineType=cv2.LINE_AA)\n",
    "        \n",
    "    except : # 얼굴인식 안되면 = 사람이 없으면 \n",
    "        print(\"dd\") # 검출안되는 멈춤 현상 방지 -> 네트워크문제, 사양\n",
    "        pass;\n",
    "    \n",
    "    cv2.imshow(\"VideoFrame\", img)\n",
    "    if cv2.waitKey(33) > 0: break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 얼굴인식(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hand detection (yolov4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yolo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f1990d14c737>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0myolo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# ap = argparse.ArgumentParser()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'yolo'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "\n",
    "from yolo import YOLO\n",
    "\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument('-n', '--network', default=\"normal\", help='Network Type: normal / tiny / prn')\n",
    "# ap.add_argument('-d', '--device', default=0, help='Device to use')\n",
    "# ap.add_argument('-s', '--size', default=416, help='Size for yolo')\n",
    "# ap.add_argument('-c', '--confidence', default=0.2, help='Confidence for yolo')\n",
    "# args = ap.parse_args()\n",
    "\n",
    "# if args.network == \"normal\":\n",
    "#     print(\"loading yolo...\")\n",
    "#     yolo = YOLO(\"models/cross-hands.cfg\", \"models/cross-hands.weights\", [\"hand\"])\n",
    "# elif args.network == \"prn\":\n",
    "#     print(\"loading yolo-tiny-prn...\")\n",
    "#     yolo = YOLO(\"models/cross-hands-tiny-prn.cfg\", \"models/cross-hands-tiny-prn.weights\", [\"hand\"])\n",
    "# else:\n",
    "#     print(\"loading yolo-tiny...\")\n",
    "yolo = YOLO(\"models/cross-hands-tiny.cfg\", \"models/cross-hands-tiny.weights\", [\"hand\"])\n",
    "\n",
    "# yolo.size = int(args.size)\n",
    "# yolo.confidence = float(args.confidence)\n",
    "\n",
    "print(\"starting webcam...\")\n",
    "cv2.namedWindow(\"preview\")\n",
    "vc = cv2.VideoCapture(0)\n",
    "\n",
    "if vc.isOpened():  # try to get the first frame\n",
    "    rval, frame = vc.read()\n",
    "else:\n",
    "    rval = False\n",
    "\n",
    "while rval:\n",
    "    width, height, inference_time, results = yolo.inference(frame)\n",
    "    for detection in results:\n",
    "        id, name, confidence, x, y, w, h = detection\n",
    "        cx = x + (w / 2)\n",
    "        cy = y + (h / 2)\n",
    "\n",
    "        # draw a bounding box rectangle and label on the image\n",
    "        color = (0, 255, 255)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "        text = \"%s (%s)\" % (name, round(confidence, 2))\n",
    "        cv2.putText(frame, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, color, 2)\n",
    "\n",
    "    cv2.imshow(\"preview\", frame)\n",
    "\n",
    "    rval, frame = vc.read()\n",
    "\n",
    "    key = cv2.waitKey(20)\n",
    "    if key == 27:  # exit on ESC\n",
    "        break\n",
    "\n",
    "cv2.destroyWindow(\"preview\")\n",
    "vc.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, dlib, sys\n",
    "import numpy as np\n",
    "\n",
    "scaler = 0.3\n",
    "\n",
    "# initialize face detector and shape predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# <load video\n",
    "\n",
    "# cap = cv2.VideoCapture('samples/girl.mp4')\n",
    "try :\n",
    "    cap = cv2.VideiCapture(1) # load realtime\n",
    "except :\n",
    "    print(\"카메라 로딩 실패\")\n",
    "\n",
    "    \n",
    "# load overlay image\n",
    "overlay = cv2.imread('samples/ryan_transparent.png', cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# overlay function\n",
    "def overlay_transparent(background_img, img_to_overlay_t, x, y, overlay_size=None):\n",
    "  bg_img = background_img.copy()\n",
    "  # convert 3 channels to 4 channels\n",
    "  if bg_img.shape[2] == 3:\n",
    "    bg_img = cv2.cvtColor(bg_img, cv2.COLOR_BGR2BGRA)\n",
    "\n",
    "  if overlay_size is not None:\n",
    "    img_to_overlay_t = cv2.resize(img_to_overlay_t.copy(), overlay_size)\n",
    "\n",
    "  b, g, r, a = cv2.split(img_to_overlay_t)\n",
    "\n",
    "  mask = cv2.medianBlur(a, 5)\n",
    "\n",
    "  h, w, _ = img_to_overlay_t.shape\n",
    "  roi = bg_img[int(y-h/2):int(y+h/2), int(x-w/2):int(x+w/2)]\n",
    "\n",
    "  img1_bg = cv2.bitwise_and(roi.copy(), roi.copy(), mask=cv2.bitwise_not(mask))\n",
    "  img2_fg = cv2.bitwise_and(img_to_overlay_t, img_to_overlay_t, mask=mask)\n",
    "\n",
    "  bg_img[int(y-h/2):int(y+h/2), int(x-w/2):int(x+w/2)] = cv2.add(img1_bg, img2_fg)\n",
    "\n",
    "  # convert 4 channels to 4 channels\n",
    "  bg_img = cv2.cvtColor(bg_img, cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "  return bg_img\n",
    "\n",
    "face_roi = []\n",
    "face_sizes = []\n",
    "\n",
    "# loop\n",
    "while True:\n",
    "  # read frame buffer from video\n",
    "  ret, img = cap.read()\n",
    "  if not ret:\n",
    "    break\n",
    "\n",
    "  # resize frame\n",
    "  img = cv2.resize(img, (int(img.shape[1] * scaler), int(img.shape[0] * scaler)))\n",
    "  ori = img.copy()\n",
    "\n",
    "  # find faces\n",
    "  if len(face_roi) == 0:\n",
    "    faces = detector(img, 1)\n",
    "  else:\n",
    "    roi_img = img[face_roi[0]:face_roi[1], face_roi[2]:face_roi[3]]\n",
    "    # cv2.imshow('roi', roi_img)\n",
    "    faces = detector(roi_img)\n",
    "\n",
    "  # no faces\n",
    "  if len(faces) == 0:\n",
    "    print('no faces!')\n",
    "\n",
    "  # find facial landmarks\n",
    "  for face in faces:\n",
    "    if len(face_roi) == 0:\n",
    "      dlib_shape = predictor(img, face)\n",
    "      shape_2d = np.array([[p.x, p.y] for p in dlib_shape.parts()])\n",
    "    else:\n",
    "      dlib_shape = predictor(roi_img, face)\n",
    "      shape_2d = np.array([[p.x + face_roi[2], p.y + face_roi[0]] for p in dlib_shape.parts()])\n",
    "\n",
    "    for s in shape_2d:\n",
    "      cv2.circle(img, center=tuple(s), radius=1, color=(255, 255, 255), thickness=2, lineType=cv2.LINE_AA)\n",
    "\n",
    "    # compute face center\n",
    "    center_x, center_y = np.mean(shape_2d, axis=0).astype(np.int)\n",
    "\n",
    "    # compute face boundaries\n",
    "    min_coords = np.min(shape_2d, axis=0)\n",
    "    max_coords = np.max(shape_2d, axis=0)\n",
    "\n",
    "    # draw min, max coords\n",
    "    cv2.circle(img, center=tuple(min_coords), radius=1, color=(255, 0, 0), thickness=2, lineType=cv2.LINE_AA)\n",
    "    cv2.circle(img, center=tuple(max_coords), radius=1, color=(255, 0, 0), thickness=2, lineType=cv2.LINE_AA)\n",
    "\n",
    "    # compute face size\n",
    "    face_size = max(max_coords - min_coords)\n",
    "    face_sizes.append(face_size)\n",
    "    if len(face_sizes) > 10:\n",
    "      del face_sizes[0]\n",
    "    mean_face_size = int(np.mean(face_sizes) * 1.8)\n",
    "\n",
    "    # compute face roi\n",
    "    face_roi = np.array([int(min_coords[1] - face_size / 2), int(max_coords[1] + face_size / 2), int(min_coords[0] - face_size / 2), int(max_coords[0] + face_size / 2)])\n",
    "    face_roi = np.clip(face_roi, 0, 10000)\n",
    "\n",
    "    # draw overlay on face\n",
    "    result = overlay_transparent(ori, overlay, center_x + 8, center_y - 25, overlay_size=(mean_face_size, mean_face_size))\n",
    "\n",
    "  # visualize\n",
    "  cv2.imshow('original', ori)\n",
    "  cv2.imshow('facial landmarks', img)\n",
    "  cv2.imshow('result', result)\n",
    "  if cv2.waitKey(1) == ord('q'):\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui as pag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1177 350\n"
     ]
    }
   ],
   "source": [
    "x, y = pag.position()\n",
    "print(x, y)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "position함수를 사용하여 (x, y) 형태의 좌표로 출력할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pag.moveTo(0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "픽셀 단위의 절대 좌표를 기준으로 마우스를 이동시켜줄 수 있다.\n",
    "* 모니터좌측상단 (0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
